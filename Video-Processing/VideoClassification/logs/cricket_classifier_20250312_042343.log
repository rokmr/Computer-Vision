src/data.py:14 - 2025-03-12 04:23:43 - INFO - Initializing VideoDatasetProcessor with dataset_id=rokmr/cricket-shot, filename=cricketshot.tar.gz
src/data.py:29 - 2025-03-12 04:23:43 - INFO - Creating directories for data processing
src/data.py:41 - 2025-03-12 04:23:43 - INFO - Found 10 classes: cover, defense, flick, hook, late_cut, lofted, pull, square_cut, straight, sweep
src/embed_lstm.py:245 - 2025-03-12 04:23:43 - INFO - Args: Namespace(model_id='openai/clip-vit-base-patch32', embed_size=512, create_embeddings=False, lr=0.001, save_model_path='./clip-cricket-classifier.pt', num_epochs=50)
src/embed_lstm.py:26 - 2025-03-12 04:23:43 - INFO - Loading model: openai/clip-vit-base-patch32
src/embed_lstm.py:94 - 2025-03-12 04:23:52 - INFO - Loading embeddings and labels for ./data/openai/clip-vit-base-patch32/frames_embeddings/train
src/embed_lstm.py:98 - 2025-03-12 04:23:52 - INFO - Found 10 classes
src/embed_lstm.py:94 - 2025-03-12 04:23:54 - INFO - Loading embeddings and labels for ./data/openai/clip-vit-base-patch32/frames_embeddings/val
src/embed_lstm.py:98 - 2025-03-12 04:23:54 - INFO - Found 10 classes
src/embed_lstm.py:94 - 2025-03-12 04:23:55 - INFO - Loading embeddings and labels for ./data/openai/clip-vit-base-patch32/frames_embeddings/test
src/embed_lstm.py:98 - 2025-03-12 04:23:55 - INFO - Found 10 classes
src/embed_lstm.py:118 - 2025-03-12 04:23:55 - INFO - Train embeddings shape: torch.Size([15714, 512]) || Val embeddings shape: torch.Size([3213, 512]) || Test embeddings shape: torch.Size([3294, 512])
src/embed_lstm.py:217 - 2025-03-12 04:23:55 - INFO - Loading data: training samples: 15714 || validation samples: 3213 || test samples: 3294
src/embed_lstm.py:222 - 2025-03-12 04:23:55 - INFO - Initializing LSTM Network: input size: 512 || hidden size: 256 || number of classes: 10
src/embed_lstm.py:141 - 2025-03-12 04:23:55 - INFO - Training model for 50 epochs
src/embed_lstm.py:170 - 2025-03-12 04:23:57 - INFO - Epoch 1 || Training Loss: 1.584483 || Val Acc.: 0.48
src/embed_lstm.py:173 - 2025-03-12 04:23:57 - INFO - Saving model at epoch 1 with val accuracy: 0.48
src/embed_lstm.py:170 - 2025-03-12 04:23:58 - INFO - Epoch 2 || Training Loss: 1.205414 || Val Acc.: 0.49
src/embed_lstm.py:173 - 2025-03-12 04:23:58 - INFO - Saving model at epoch 2 with val accuracy: 0.49
src/embed_lstm.py:170 - 2025-03-12 04:24:00 - INFO - Epoch 3 || Training Loss: 0.996789 || Val Acc.: 0.49
src/embed_lstm.py:170 - 2025-03-12 04:24:01 - INFO - Epoch 4 || Training Loss: 0.830428 || Val Acc.: 0.53
src/embed_lstm.py:173 - 2025-03-12 04:24:01 - INFO - Saving model at epoch 4 with val accuracy: 0.53
src/embed_lstm.py:170 - 2025-03-12 04:24:03 - INFO - Epoch 5 || Training Loss: 0.690234 || Val Acc.: 0.54
src/embed_lstm.py:173 - 2025-03-12 04:24:03 - INFO - Saving model at epoch 5 with val accuracy: 0.54
src/embed_lstm.py:170 - 2025-03-12 04:24:04 - INFO - Epoch 6 || Training Loss: 0.574578 || Val Acc.: 0.54
src/embed_lstm.py:170 - 2025-03-12 04:24:06 - INFO - Epoch 7 || Training Loss: 0.474130 || Val Acc.: 0.55
src/embed_lstm.py:173 - 2025-03-12 04:24:06 - INFO - Saving model at epoch 7 with val accuracy: 0.55
src/embed_lstm.py:170 - 2025-03-12 04:24:08 - INFO - Epoch 8 || Training Loss: 0.396413 || Val Acc.: 0.55
src/embed_lstm.py:170 - 2025-03-12 04:24:10 - INFO - Epoch 9 || Training Loss: 0.325588 || Val Acc.: 0.55
src/embed_lstm.py:170 - 2025-03-12 04:24:11 - INFO - Epoch 10 || Training Loss: 0.282076 || Val Acc.: 0.55
src/embed_lstm.py:173 - 2025-03-12 04:24:11 - INFO - Saving model at epoch 10 with val accuracy: 0.55
src/embed_lstm.py:170 - 2025-03-12 04:24:13 - INFO - Epoch 11 || Training Loss: 0.231421 || Val Acc.: 0.56
src/embed_lstm.py:173 - 2025-03-12 04:24:13 - INFO - Saving model at epoch 11 with val accuracy: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:24:14 - INFO - Epoch 12 || Training Loss: 0.202127 || Val Acc.: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:24:16 - INFO - Epoch 13 || Training Loss: 0.173232 || Val Acc.: 0.56
src/embed_lstm.py:173 - 2025-03-12 04:24:16 - INFO - Saving model at epoch 13 with val accuracy: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:24:17 - INFO - Epoch 14 || Training Loss: 0.148186 || Val Acc.: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:24:19 - INFO - Epoch 15 || Training Loss: 0.119859 || Val Acc.: 0.57
src/embed_lstm.py:173 - 2025-03-12 04:24:19 - INFO - Saving model at epoch 15 with val accuracy: 0.57
src/embed_lstm.py:170 - 2025-03-12 04:24:20 - INFO - Epoch 16 || Training Loss: 0.106254 || Val Acc.: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:24:22 - INFO - Epoch 17 || Training Loss: 0.100010 || Val Acc.: 0.55
src/embed_lstm.py:170 - 2025-03-12 04:24:23 - INFO - Epoch 18 || Training Loss: 0.081567 || Val Acc.: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:24:25 - INFO - Epoch 19 || Training Loss: 0.072713 || Val Acc.: 0.55
src/embed_lstm.py:170 - 2025-03-12 04:24:26 - INFO - Epoch 20 || Training Loss: 0.069805 || Val Acc.: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:24:28 - INFO - Epoch 21 || Training Loss: 0.056758 || Val Acc.: 0.57
src/embed_lstm.py:170 - 2025-03-12 04:24:30 - INFO - Epoch 22 || Training Loss: 0.055832 || Val Acc.: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:24:31 - INFO - Epoch 23 || Training Loss: 0.059803 || Val Acc.: 0.57
src/embed_lstm.py:173 - 2025-03-12 04:24:31 - INFO - Saving model at epoch 23 with val accuracy: 0.57
src/embed_lstm.py:170 - 2025-03-12 04:24:33 - INFO - Epoch 24 || Training Loss: 0.044002 || Val Acc.: 0.55
src/embed_lstm.py:170 - 2025-03-12 04:24:34 - INFO - Epoch 25 || Training Loss: 0.041688 || Val Acc.: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:24:36 - INFO - Epoch 26 || Training Loss: 0.040807 || Val Acc.: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:24:37 - INFO - Epoch 27 || Training Loss: 0.052711 || Val Acc.: 0.57
src/embed_lstm.py:170 - 2025-03-12 04:24:39 - INFO - Epoch 28 || Training Loss: 0.028873 || Val Acc.: 0.53
src/embed_lstm.py:170 - 2025-03-12 04:24:40 - INFO - Epoch 29 || Training Loss: 0.039230 || Val Acc.: 0.57
src/embed_lstm.py:173 - 2025-03-12 04:24:40 - INFO - Saving model at epoch 29 with val accuracy: 0.57
src/embed_lstm.py:170 - 2025-03-12 04:24:42 - INFO - Epoch 30 || Training Loss: 0.042895 || Val Acc.: 0.55
src/embed_lstm.py:170 - 2025-03-12 04:24:43 - INFO - Epoch 31 || Training Loss: 0.030154 || Val Acc.: 0.54
src/embed_lstm.py:170 - 2025-03-12 04:24:45 - INFO - Epoch 32 || Training Loss: 0.032187 || Val Acc.: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:24:46 - INFO - Epoch 33 || Training Loss: 0.032188 || Val Acc.: 0.55
src/embed_lstm.py:170 - 2025-03-12 04:24:48 - INFO - Epoch 34 || Training Loss: 0.023436 || Val Acc.: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:24:50 - INFO - Epoch 35 || Training Loss: 0.042410 || Val Acc.: 0.55
src/embed_lstm.py:170 - 2025-03-12 04:24:51 - INFO - Epoch 36 || Training Loss: 0.016200 || Val Acc.: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:24:53 - INFO - Epoch 37 || Training Loss: 0.051930 || Val Acc.: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:24:54 - INFO - Epoch 38 || Training Loss: 0.018854 || Val Acc.: 0.58
src/embed_lstm.py:173 - 2025-03-12 04:24:54 - INFO - Saving model at epoch 38 with val accuracy: 0.58
src/embed_lstm.py:170 - 2025-03-12 04:24:56 - INFO - Epoch 39 || Training Loss: 0.003476 || Val Acc.: 0.58
src/embed_lstm.py:173 - 2025-03-12 04:24:56 - INFO - Saving model at epoch 39 with val accuracy: 0.58
src/embed_lstm.py:170 - 2025-03-12 04:24:57 - INFO - Epoch 40 || Training Loss: 0.067926 || Val Acc.: 0.57
src/embed_lstm.py:170 - 2025-03-12 04:24:59 - INFO - Epoch 41 || Training Loss: 0.009791 || Val Acc.: 0.57
src/embed_lstm.py:170 - 2025-03-12 04:25:01 - INFO - Epoch 42 || Training Loss: 0.011456 || Val Acc.: 0.55
src/embed_lstm.py:170 - 2025-03-12 04:25:02 - INFO - Epoch 43 || Training Loss: 0.060527 || Val Acc.: 0.55
src/embed_lstm.py:170 - 2025-03-12 04:25:04 - INFO - Epoch 44 || Training Loss: 0.014605 || Val Acc.: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:25:05 - INFO - Epoch 45 || Training Loss: 0.037787 || Val Acc.: 0.57
src/embed_lstm.py:170 - 2025-03-12 04:25:07 - INFO - Epoch 46 || Training Loss: 0.007248 || Val Acc.: 0.58
src/embed_lstm.py:170 - 2025-03-12 04:25:08 - INFO - Epoch 47 || Training Loss: 0.054277 || Val Acc.: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:25:10 - INFO - Epoch 48 || Training Loss: 0.019565 || Val Acc.: 0.56
src/embed_lstm.py:170 - 2025-03-12 04:25:11 - INFO - Epoch 49 || Training Loss: 0.003521 || Val Acc.: 0.57
src/embed_lstm.py:170 - 2025-03-12 04:25:13 - INFO - Epoch 50 || Training Loss: 0.040985 || Val Acc.: 0.52
src/embed_lstm.py:178 - 2025-03-12 04:25:13 - INFO - Best Model Saved ./clip-cricket-classifier.pt: Max Val Acc. 0.58 at epoch 39
src/embed_lstm.py:182 - 2025-03-12 04:25:13 - INFO - Testing model
src/embed_lstm.py:200 - 2025-03-12 04:25:13 - INFO - Test Acc.: 0.52
src/embed_lstm.py:202 - 2025-03-12 04:25:13 - INFO - Confusion Matrix:
src/embed_lstm.py:203 - 2025-03-12 04:25:13 - INFO - [[ 88   0   4  44  23  21  17  28  31  17]
 [  1 354   1  11  12   0   0   2  15   1]
 [ 28  10  77  41  22  11  13  33  11  14]
 [  7   1   0 214  13  14   7   2  10  12]
 [ 21   2   3  25 152  15  14   4  20  26]
 [ 17   0  21  84  56 281  43  13  19  38]
 [ 29   0  19  33  12  22  92  15  12  13]
 [ 38   0  34  45  22  36   9  56  10  42]
 [ 17   5   8  42  36  12   4  23 221   5]
 [ 19   4  12  49   4  29  21   8   5 167]]
src/embed_lstm.py:206 - 2025-03-12 04:25:13 - INFO - Classification Report:
src/embed_lstm.py:207 - 2025-03-12 04:25:13 - INFO -               precision    recall  f1-score   support

       cover       0.33      0.32      0.33       273
     defense       0.94      0.89      0.92       397
       flick       0.43      0.30      0.35       260
        hook       0.36      0.76      0.49       280
    late_cut       0.43      0.54      0.48       282
      lofted       0.64      0.49      0.55       572
        pull       0.42      0.37      0.39       247
  square_cut       0.30      0.19      0.24       292
    straight       0.62      0.59      0.61       373
       sweep       0.50      0.53      0.51       318

    accuracy                           0.52      3294
   macro avg       0.50      0.50      0.49      3294
weighted avg       0.53      0.52      0.51      3294

